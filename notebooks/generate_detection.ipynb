{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 42/42 [00:30<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output images saved to output_frames/video05/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# Define your bounding box color (e.g., red)\n",
    "# Convert BGR to RGBA format (0-1 range)\n",
    "bounding_box_color = (0, 0, 255)  # BGR format for red\n",
    "bounding_box_color_rgb = (bounding_box_color[2] / 255.0, bounding_box_color[1] / 255.0, bounding_box_color[0] / 255.0, 1.0)\n",
    "\n",
    "# Extend Visualizer to customize color handling\n",
    "class MyVisualizer(Visualizer):\n",
    "    def __init__(self, img_rgb, metadata, *args, **kwargs):\n",
    "        super().__init__(img_rgb, metadata, *args, **kwargs)\n",
    "    \n",
    "    def draw_instance_predictions(self, predictions):\n",
    "        for instance in predictions.pred_boxes:\n",
    "            self.draw_box(instance.to(\"cpu\"), edge_color=bounding_box_color_rgb)\n",
    "        return self.output\n",
    "    \n",
    "# Load tire annotations\n",
    "def load_tire_annotations(xml_path):\n",
    "    tire_annotations = {}\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    for track in root.findall('.//track'):\n",
    "        label = track.attrib['label']\n",
    "        if label == \"tire\":\n",
    "            for box in track.findall('.//box'):\n",
    "                frame_num = int(box.attrib['frame'])\n",
    "                if frame_num not in tire_annotations:\n",
    "                    tire_annotations[frame_num] = []\n",
    "                tire_annotations[frame_num].append(box.attrib)\n",
    "    return tire_annotations\n",
    "\n",
    "# Configure the model\n",
    "cfg = get_cfg()\n",
    "config_file = \"/home/isabelle.melo/proc/Mosquitoes/faster_detectron2/configs/faster_rcnn_R_50_FPN_1x/faster_rcnn_R_50_FPN_1x.yaml\"\n",
    "cfg.merge_from_file(config_file)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set the testing threshold for this model\n",
    "cfg.MODEL.WEIGHTS = \"outputs/faster_rcnn_R_50_FPN_1x/teste_augmentation/fold1_tire/model_early_stop.pth\"  # path to the model we just trained\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Only one class\n",
    "cfg.DATASETS.TRAIN = ('mbg_train1_tire',)\n",
    "cfg.DATASETS.TEST = (\"mbg_val1_tire\", )\n",
    "\n",
    "# Register dataset\n",
    "MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes = [\"tire\"]\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = '../dataset/v1/frames/video05/'\n",
    "output_dir = 'output_frames/video05/'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get the list of image files\n",
    "image_files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "\n",
    "# Process each image in the input directory\n",
    "for filename in tqdm(image_files, desc=\"Processing images\"):\n",
    "    input_path = os.path.join(input_dir, filename)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(input_path)\n",
    "    \n",
    "    # Make predictions\n",
    "    outputs = predictor(image)\n",
    "    \n",
    "    # Check if there are any predictions\n",
    "    if len(outputs[\"instances\"].pred_boxes) > 0:\n",
    "        # Create custom visualizer\n",
    "        v = MyVisualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2, instance_mode=ColorMode.SEGMENTATION)\n",
    "        \n",
    "        # Draw predictions on the image\n",
    "        out_image = v.draw_instance_predictions(outputs[\"instances\"]).get_image()[:, :, ::-1]\n",
    "        \n",
    "        # Save the output image\n",
    "        cv2.imwrite(output_path, out_image)\n",
    "\n",
    "print(f\"Output images saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# Define your bounding box color (e.g., red)\n",
    "# Convert BGR to RGBA format (0-1 range)\n",
    "bounding_box_color = (0, 0, 255)  # BGR format for red\n",
    "bounding_box_color_rgb = (bounding_box_color[2] / 255.0, bounding_box_color[1] / 255.0, bounding_box_color[0] / 255.0, 1.0)\n",
    "\n",
    "# Extend Visualizer to customize color handling\n",
    "class MyVisualizer(Visualizer):\n",
    "    def __init__(self, img_rgb, metadata, *args, **kwargs):\n",
    "        super().__init__(img_rgb, metadata, *args, **kwargs)\n",
    "    \n",
    "    def draw_instance_predictions(self, predictions):\n",
    "        for instance in predictions.pred_boxes:\n",
    "            self.draw_box(instance.to(\"cpu\"), edge_color=bounding_box_color_rgb)\n",
    "        return self.output\n",
    "    \n",
    "# Load tire annotations\n",
    "def load_tire_annotations(xml_path):\n",
    "    tire_annotations = {}\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    for track in root.findall('.//track'):\n",
    "        label = track.attrib['label']\n",
    "        if label == \"tire\":\n",
    "            for box in track.findall('.//box'):\n",
    "                frame_num = int(box.attrib['frame'])\n",
    "                if frame_num not in tire_annotations:\n",
    "                    tire_annotations[frame_num] = []\n",
    "                tire_annotations[frame_num].append(box.attrib)\n",
    "    return tire_annotations\n",
    "\n",
    "# Configure the model\n",
    "cfg = get_cfg()\n",
    "config_file = \"/home/isabelle.melo/proc/Mosquitoes/faster_detectron2/configs/faster_rcnn_R_50_FPN_1x/faster_rcnn_R_50_FPN_1x.yaml\"\n",
    "cfg.merge_from_file(config_file)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set the testing threshold for this model\n",
    "cfg.MODEL.WEIGHTS = \"outputs/faster_rcnn_R_50_FPN_1x/teste_augmentation/fold1_tire/model_early_stop.pth\"  # path to the model we just trained\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Only one class\n",
    "cfg.DATASETS.TRAIN = ('mbg_train1_tire',)\n",
    "cfg.DATASETS.TEST = (\"mbg_val1_tire\", )\n",
    "\n",
    "# Register dataset\n",
    "MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes = [\"tire\"]\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = '../dataset/v1/frames/video04/'\n",
    "output_dir = 'output_frames/video04/'\n",
    "annot_path = '../dataset/v1/annotations-xml/video04.xml'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get the list of image files\n",
    "image_files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "\n",
    "# Process each image in the input directory\n",
    "for filename in tqdm(image_files, desc=\"Processing images\"):\n",
    "    input_path = os.path.join(input_dir, filename)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(input_path)\n",
    "    \n",
    "    # Make predictions\n",
    "    outputs = predictor(image)\n",
    "    \n",
    "    tire_annotations = load_tire_annotations(annot_path)\n",
    "    if filename in tire_annotations:\n",
    "        for box_info in tire_annotations[filename]:\n",
    "            xtl = int(float(box_info['xtl']))\n",
    "            ytl = int(float(box_info['ytl']))\n",
    "            xbr = int(float(box_info['xbr']))\n",
    "            ybr = int(float(box_info['ybr']))\n",
    "            thickness = 3\n",
    "            cv2.rectangle(image, (xtl, ytl), (xbr, ybr), (0, 255, 0), thickness)\n",
    "    \n",
    "    # Check if there are any predictions\n",
    "    if len(outputs[\"instances\"].pred_boxes) > 0:\n",
    "        # Create custom visualizer\n",
    "        v = MyVisualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2, instance_mode=ColorMode.SEGMENTATION)\n",
    "        \n",
    "        # Draw predictions on the image\n",
    "        out_image = v.draw_instance_predictions(outputs[\"instances\"]).get_image()[:, :, ::-1]\n",
    "        \n",
    "        # Save the output image\n",
    "        cv2.imwrite(output_path, out_image)\n",
    "\n",
    "print(f\"Output images saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   6%|▌         | 13/211 [00:05<01:16,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 2760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  32%|███▏      | 67/211 [00:24<00:50,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 2952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  38%|███▊      | 81/211 [00:29<00:43,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 2976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  39%|███▉      | 82/211 [00:29<00:45,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  39%|███▉      | 83/211 [00:30<00:41,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 3024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  71%|███████   | 149/211 [00:53<00:21,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  71%|███████   | 150/211 [00:55<00:39,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 4248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  72%|███████▏  | 151/211 [00:56<00:51,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 4272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  73%|███████▎  | 153/211 [00:58<00:48,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 2784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  77%|███████▋  | 162/211 [01:02<00:20,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 4776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  77%|███████▋  | 163/211 [01:04<00:40,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  78%|███████▊  | 164/211 [01:06<00:50,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 4824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  78%|███████▊  | 165/211 [01:07<00:57,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  79%|███████▊  | 166/211 [01:09<00:59,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 4872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  79%|███████▉  | 167/211 [01:10<01:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 4896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  94%|█████████▍| 198/211 [01:23<00:03,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 2808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  94%|█████████▍| 199/211 [01:24<00:07,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 2832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  95%|█████████▍| 200/211 [01:26<00:09,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 2856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  95%|█████████▌| 201/211 [01:27<00:09,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_num: 2880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 211/211 [01:31<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output images saved to output_frames/video13/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from xml.etree import ElementTree as ET\n",
    "import re\n",
    "\n",
    "# Define your bounding box color (e.g., red)\n",
    "# Convert BGR to RGBA format (0-1 range)\n",
    "bounding_box_color = (0, 0, 255)  # BGR format for red\n",
    "bounding_box_color_rgb = (bounding_box_color[2] / 255.0, bounding_box_color[1] / 255.0, bounding_box_color[0] / 255.0, 1.0)\n",
    "\n",
    "# Extend Visualizer to customize color handling\n",
    "class MyVisualizer(Visualizer):\n",
    "    def __init__(self, img_rgb, metadata, *args, **kwargs):\n",
    "        super().__init__(img_rgb, metadata, *args, **kwargs)\n",
    "    \n",
    "    def draw_instance_predictions(self, predictions):\n",
    "        for instance in predictions.pred_boxes:\n",
    "            self.draw_box(instance.to(\"cpu\"), edge_color=bounding_box_color_rgb)\n",
    "        return self.output\n",
    "    \n",
    "# Load tire annotations\n",
    "def load_tire_annotations(xml_path):\n",
    "    tire_annotations = {}\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    for track in root.findall('.//track'):\n",
    "        label = track.attrib['label']\n",
    "        if label == \"tire\":\n",
    "            for box in track.findall('.//box'):\n",
    "                frame_num = int(box.attrib['frame'])\n",
    "                if frame_num not in tire_annotations:\n",
    "                    tire_annotations[frame_num] = []\n",
    "                tire_annotations[frame_num].append(box.attrib)\n",
    "    return tire_annotations\n",
    "\n",
    "# Configure the model\n",
    "cfg = get_cfg()\n",
    "config_file = \"/home/isabelle.melo/proc/Mosquitoes/faster_detectron2/configs/faster_rcnn_R_50_FPN_1x/faster_rcnn_R_50_FPN_1x.yaml\"\n",
    "cfg.merge_from_file(config_file)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9  # set the testing threshold for this model\n",
    "cfg.MODEL.WEIGHTS = \"outputs/faster_rcnn_R_50_FPN_1x/teste_augmentation/fold1_tire/model_early_stop.pth\"  # path to the model we just trained\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Only one class\n",
    "cfg.DATASETS.TRAIN = ('mbg_train1_tire',)\n",
    "cfg.DATASETS.TEST = (\"mbg_val1_tire\", )\n",
    "\n",
    "# Register dataset\n",
    "MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes = [\"tire\"]\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = '../dataset/v1/frames/video13/'\n",
    "output_dir = 'output_frames/video13/'\n",
    "annot_path = '../dataset/v1/annotations-xml/video13.xml'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get the list of image files\n",
    "image_files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "\n",
    "# Regular expression pattern to match numbers in filenames\n",
    "pattern = re.compile(r'frame_(\\d+)\\.png')\n",
    "\n",
    "# Process each image in the input directory\n",
    "for filename in tqdm(image_files, desc=\"Processing images\"):\n",
    "    input_path = os.path.join(input_dir, filename)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Extract the frame number from the filename\n",
    "    match = pattern.match(filename)\n",
    "    frame_num = int(match.group(1)) if match else None\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(input_path)\n",
    "    \n",
    "    # Make predictions\n",
    "    outputs = predictor(image)\n",
    "    \n",
    "    tire_annotations = load_tire_annotations(annot_path)\n",
    "    if frame_num in tire_annotations:\n",
    "        print(f'frame_num: {frame_num}')\n",
    "        for box_info in tire_annotations[frame_num]:\n",
    "            xtl = int(float(box_info['xtl']))\n",
    "            ytl = int(float(box_info['ytl']))\n",
    "            xbr = int(float(box_info['xbr']))\n",
    "            ybr = int(float(box_info['ybr']))\n",
    "            thickness = 5\n",
    "            cv2.rectangle(image, (xtl, ytl), (xbr, ybr), (255, 0, 0), thickness)\n",
    "    \n",
    "    # Check if there are any predictions\n",
    "    if len(outputs[\"instances\"].pred_boxes) > 0:\n",
    "        # Create custom visualizer\n",
    "        v = MyVisualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2, instance_mode=ColorMode.SEGMENTATION)\n",
    "        \n",
    "        # Draw predictions on the image\n",
    "        out_image = v.draw_instance_predictions(outputs[\"instances\"]).get_image()[:, :, ::-1]\n",
    "        \n",
    "        # Save the output image\n",
    "        cv2.imwrite(output_path, out_image)\n",
    "\n",
    "print(f\"Output images saved to {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mukitos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
